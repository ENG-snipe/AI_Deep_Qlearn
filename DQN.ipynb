{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stable_baselines3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DQN\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DummyVecEnv\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvalCallback\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.envs import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "class TwoLinkArmEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(TwoLinkArmEnv, self).__init__()\n",
    "        self.link1_length = 1.0  # Length of the first link\n",
    "        self.link2_length = 1.0  # Length of the second link\n",
    "        self.max_angle = np.pi  # Maximum angle (in radians)\n",
    "        \n",
    "        # Action space: discrete action space of 10 levels per joint (discretized joint angles)\n",
    "        self.num_actions = 10\n",
    "        self.action_space = spaces.Discrete(self.num_actions * self.num_actions)\n",
    "\n",
    "        # Observation space: two continuous values for joint angles (theta1, theta2)\n",
    "        self.observation_space = spaces.Box(low=-self.max_angle, high=self.max_angle, shape=(2,), dtype=np.float32)\n",
    "\n",
    "        # Initial state: joint angles (theta1, theta2)\n",
    "        self.state = np.zeros(2, dtype=np.float32)\n",
    "\n",
    "        # Initialize the plot for visualization\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "        self.ax.set_xlim(-2, 2)\n",
    "        self.ax.set_ylim(-2, 2)\n",
    "        self.ax.set_aspect('equal')\n",
    "\n",
    "        # Discretize the joint angles\n",
    "        self.angle_values = np.linspace(-self.max_angle, self.max_angle, self.num_actions)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Convert the discrete action to the corresponding joint angles\n",
    "        theta1_index = action // self.num_actions\n",
    "        theta2_index = action % self.num_actions\n",
    "        theta1 = self.angle_values[theta1_index]\n",
    "        theta2 = self.angle_values[theta2_index]\n",
    "        \n",
    "        self.state = np.array([theta1, theta2], dtype=np.float32)\n",
    "\n",
    "        # Calculate the end-effector position\n",
    "        x1 = self.link1_length * np.cos(self.state[0])\n",
    "        y1 = self.link1_length * np.sin(self.state[0])\n",
    "        x2 = x1 + self.link2_length * np.cos(self.state[0] + self.state[1])\n",
    "        y2 = y1 + self.link2_length * np.sin(self.state[0] + self.state[1])\n",
    "\n",
    "        # Calculate the reward (negative distance to origin for simplicity)\n",
    "        reward = -np.sqrt(x2**2 + y2**2)\n",
    "\n",
    "        # Termination condition (no termination in this case)\n",
    "        done = False\n",
    "        \n",
    "        return np.copy(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state to the initial position\n",
    "        self.state = np.zeros(2, dtype=np.float32)\n",
    "        return np.copy(self.state)\n",
    "\n",
    "    def render(self):\n",
    "        # Calculate the joint positions for visualization\n",
    "        x1 = self.link1_length * np.cos(self.state[0])\n",
    "        y1 = self.link1_length * np.sin(self.state[0])\n",
    "        x2 = x1 + self.link2_length * np.cos(self.state[0] + self.state[1])\n",
    "        y2 = y1 + self.link2_length * np.sin(self.state[0] + self.state[1])\n",
    "\n",
    "        # Plot the arm\n",
    "        self.ax.clear()\n",
    "        self.ax.set_xlim(-2, 2)\n",
    "        self.ax.set_ylim(-2, 2)\n",
    "        self.ax.set_aspect('equal')\n",
    "\n",
    "        # Plot the first link\n",
    "        self.ax.plot([0, x1], [0, y1], lw=4, color='blue')\n",
    "        # Plot the second link\n",
    "        self.ax.plot([x1, x2], [y1, y2], lw=4, color='green')\n",
    "\n",
    "        # Plot the end effector (tip of the second link)\n",
    "        self.ax.plot(x2, y2, 'ro')\n",
    "\n",
    "        # Display the plot\n",
    "        plt.draw()\n",
    "        plt.pause(0.01)\n",
    "\n",
    "    def close(self):\n",
    "        # Close the environment\n",
    "        plt.close(self.fig)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = DummyVecEnv([lambda: TwoLinkArmEnv()])\n",
    "\n",
    "# Initialize DQN agent\n",
    "model = DQN('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"dqn_two_link_arm\")\n",
    "\n",
    "# Evaluate the agent\n",
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action, _state = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
